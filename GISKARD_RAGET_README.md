# Ã‰valuation du RAG Flowkura avec Giskard RAGET

Ce document explique comment Ã©valuer le chat **Diplomeo dev** sur Ragflow en utilisant **Giskard RAGET** pour gÃ©nÃ©rer automatiquement un testset de questions/rÃ©ponses.

## ğŸ¯ Objectif

GÃ©nÃ©rer automatiquement un **testset d'Ã©valuation** Ã  partir de la base de connaissances `ragflow-sample/` et l'utiliser pour tester le chat Diplomeo dev de maniÃ¨re systÃ©matique.

## ğŸ“¦ PrÃ©requis

### 1. Installation de Giskard

```bash
# Installation (peut prendre 5-10 minutes, tÃ©lÃ©charge ~1GB de dÃ©pendances)
pip install -r requirements-giskard.txt
```

**Note**: Giskard installe PyTorch et de nombreuses dÃ©pendances ML. Si vous avez dÃ©jÃ  PyTorch installÃ©, vous pouvez installer uniquement :
```bash
pip install giskard
```

### 2. Configuration OpenAI

RAGET utilise un LLM pour gÃ©nÃ©rer les questions/rÃ©ponses. Vous devez configurer votre clÃ© API OpenAI :

```bash
export OPENAI_API_KEY='votre-clÃ©-openai'
```

Vous pouvez aussi utiliser d'autres LLMs supportÃ©s par Giskard (Anthropic Claude, Mistral, etc.).

## ğŸš€ Utilisation

### Script principal : `giskard_raget_evaluation.py`

```bash
python giskard_raget_evaluation.py
```

Le script va :

1. **Se connecter au chat Diplomeo dev** sur Ragflow
2. **Charger la base de connaissances** depuis `ragflow-sample/` :
   - `1_metiers/` : 9 fiches mÃ©tiers
   - `2_formations/` : 12 formations
   - `3_actions_formation/` : 41 actions de formation
   - `4_etablissements/` : Ã‰tablissements
3. **GÃ©nÃ©rer un testset avec RAGET** (20 questions par dÃ©faut)
4. **Ã‰valuer le chat** en lui posant toutes les questions
5. **Analyser les rÃ©sultats** et gÃ©nÃ©rer un rapport

### RÃ©sultats

Tous les rÃ©sultats sont sauvegardÃ©s dans `giskard_results/` :

```
giskard_results/
â”œâ”€â”€ testset_20260204_HHMMSS.jsonl          # Testset rÃ©utilisable
â”œâ”€â”€ evaluation_results_20260204_HHMMSS.csv # RÃ©sultats dÃ©taillÃ©s
â”œâ”€â”€ stats_20260204_HHMMSS.json             # Statistiques
â””â”€â”€ report_20260204_HHMMSS.md              # Rapport Markdown
```

## ğŸ“Š Que fait RAGET ?

**RAGET** (RAG Evaluation Toolkit) de Giskard gÃ©nÃ¨re automatiquement diffÃ©rents types de questions :

### Types de questions gÃ©nÃ©rÃ©es

1. **Questions simples** : Questions directes sur le contenu
   - Exemple : "Quels sont les prÃ©requis pour le BTS ComptabilitÃ© ?"

2. **Questions complexes** : Questions nÃ©cessitant plusieurs sources
   - Exemple : "Quelles formations en comptabilitÃ© sont disponibles Ã  Lille ?"

3. **Questions de distracteurs** : Questions avec des informations trompeuses
   - Exemple : "Le BTS Informatique est-il disponible en 1 an ?" (Faux)

4. **Questions conversationnelles** : Questions avec contexte
   - Exemple : "Je cherche une formation. â†’ En comptabilitÃ©. â†’ Ã€ Lille."

5. **Questions hors contexte** : Questions sans rÃ©ponse dans la base
   - Exemple : "Quel est le salaire moyen d'un comptable ?" (Si non documentÃ©)

## ğŸ¨ Personnalisation

### Modifier le nombre de questions

Ã‰ditez le script ou passez le paramÃ¨tre :

```python
num_questions = 50  # Au lieu de 20
```

### Changer le LLM utilisÃ©

Par dÃ©faut, RAGET utilise OpenAI GPT-4. Pour utiliser un autre modÃ¨le :

```python
from giskard.llm import set_default_client
from giskard.llm.client import OpenAIClient

# Utiliser GPT-3.5 au lieu de GPT-4 (moins cher)
client = OpenAIClient(model="gpt-3.5-turbo")
set_default_client(client)
```

### Filtrer la base de connaissances

Pour ne tester que certaines catÃ©gories :

```python
# Ne charger que les formations et mÃ©tiers
knowledge_base = knowledge_base[
    knowledge_base['category'].isin([
        'Formations - Orientation France',
        'MÃ©tiers - Orientation France'
    ])
]
```

## ğŸ“ˆ MÃ©triques et analyse

Le script gÃ©nÃ¨re plusieurs mÃ©triques :

### MÃ©triques basiques

- **Total de questions** : Nombre total de questions testÃ©es
- **Longueur moyenne des rÃ©ponses** : En caractÃ¨res
- **RÃ©ponses vides** : Nombre de fois oÃ¹ le RAG n'a pas rÃ©pondu
- **RÃ©ponses avec erreur** : RÃ©ponses contenant "erreur"

### MÃ©triques avancÃ©es (si activÃ©)

Pour activer les mÃ©triques RAGET avancÃ©es, dÃ©commentez la section dans le script :

```python
# Calculer les mÃ©triques RAGET
from giskard.rag import evaluate

metrics = evaluate(
    question=results_df['question'],
    reference_answer=results_df['reference_answer'],
    answer=results_df['rag_answer'],
    reference_context=results_df['reference_context']
)
```

Ces mÃ©triques Ã©valuent :
- **Generator** : QualitÃ© des rÃ©ponses gÃ©nÃ©rÃ©es
- **Retriever** : Pertinence des documents rÃ©cupÃ©rÃ©s
- **Overall** : Score global du systÃ¨me RAG

## ğŸ”§ DÃ©pannage

### Erreur : "OPENAI_API_KEY not found"

```bash
export OPENAI_API_KEY='sk-...'
```

### Erreur : "ImportError: giskard"

```bash
pip install -r requirements-giskard.txt
```

### Installation trop longue

Giskard installe PyTorch (~1GB). C'est normal. L'installation peut prendre 5-10 minutes.

### Le testset ne se gÃ©nÃ¨re pas

- VÃ©rifiez que `ragflow-sample/` contient bien des fichiers `.md`
- VÃ©rifiez votre clÃ© OpenAI
- RÃ©duisez le nombre de questions pour tester

## ğŸ“š Ressources

- [Documentation Giskard](https://docs.giskard.ai/)
- [RAGET - RAG Evaluation Toolkit](https://docs.giskard.ai/en/stable/open_source/scan/rag_evaluation/index.html)
- [GitHub Giskard](https://github.com/Giskard-AI/giskard-oss)

## ğŸ’¡ Utilisation avancÃ©e

### RÃ©utiliser un testset existant

```python
from giskard.rag import QATestset

# Charger un testset sauvegardÃ©
testset = QATestset.load("giskard_results/testset_20260204_120000.jsonl")

# Ã‰valuer Ã  nouveau avec ce testset
results = evaluate_rag_with_testset(agent, testset)
```

### Ajouter des questions manuelles

```python
# CrÃ©er un testset personnalisÃ©
custom_questions = [
    {
        "question": "Quelles formations en comptabilitÃ© Ã  Lille ?",
        "reference_answer": "BTS ComptabilitÃ© et Gestion disponible Ã  Lille",
        "reference_context": "...",
        "metadata": {"question_type": "custom"}
    }
]

# Combiner avec le testset RAGET
# ... (voir documentation Giskard)
```

## ğŸ¯ Cas d'usage

### 1. Tester une nouvelle version du prompt

1. GÃ©nÃ©rez un testset avec la version actuelle
2. Sauvegardez les rÃ©sultats
3. Modifiez le prompt du chat
4. Re-testez avec le mÃªme testset
5. Comparez les rÃ©sultats

### 2. Tester l'ajout de nouveaux documents

1. Testset initial avec les documents actuels
2. Ajoutez de nouveaux documents Ã  Ragflow
3. Re-testez avec le mÃªme testset
4. VÃ©rifiez si les rÃ©ponses s'amÃ©liorent

### 3. Benchmarking continu

- GÃ©nÃ©rez un testset de rÃ©fÃ©rence
- Testez rÃ©guliÃ¨rement (aprÃ¨s chaque modification)
- Suivez l'Ã©volution des mÃ©triques dans le temps

## ğŸ“ Structure du code

```
giskard_raget_evaluation.py
â”œâ”€â”€ RagflowAgent                          # Wrapper pour le chat Diplomeo dev
â”‚   â”œâ”€â”€ query()                           # Interroger le chat
â”‚   â””â”€â”€ get_answer()                      # Version simplifiÃ©e
â”œâ”€â”€ load_knowledge_base_from_ragflow_sample()  # Charger ragflow-sample/
â”œâ”€â”€ generate_testset_with_raget()         # GÃ©nÃ©rer le testset
â”œâ”€â”€ evaluate_rag_with_testset()           # Ã‰valuer le RAG
â”œâ”€â”€ analyze_results()                     # Analyser les rÃ©sultats
â”œâ”€â”€ save_results()                        # Sauvegarder tout
â””â”€â”€ main()                                # Orchestration
```

---

**Bon test ! ğŸš€**
